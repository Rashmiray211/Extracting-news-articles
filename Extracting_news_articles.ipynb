{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extracting news articles.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rashmiray211/Extracting-news-articles/blob/main/Extracting_news_articles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SppaaKdnFguN"
      },
      "source": [
        "Extracting(data scraping) news articles from \"the hindu\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxTl57UGyW8O"
      },
      "source": [
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15YhOmNoyeOA"
      },
      "source": [
        "url_p=[]#List for storing the url's for different pages \n",
        "all_data=[]#List containing all data\n",
        "page_href=[]#list for storing hrefs of a page\n",
        "all_hrefs=[]#list for storing hrefs of all the pages\n",
        "j=0\n",
        "for i in range(1,80):\n",
        "  url_p.append('https://www.thehindu.com/news/national/?page='+str(i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qxXB38Gdxsh"
      },
      "source": [
        "page_href=[]#list for storing hrefs of a page\n",
        "all_hrefs=[]#list of all hrefs\n",
        "page_href1=[]\n",
        "for url in url_p:\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.content,'html.parser')\n",
        "  for d in soup.findAll('div',attrs={\"class\":\"Other-StoryCard\"}):#find all <\"class\":\"Other-StoryCard\"> tags in a page\n",
        "    for a2 in d.find_all('h3'): #find all <h3> tags in the above tags\n",
        "       for a in a2.find_all('a'):#find all <a> tags in those <h3> tags\n",
        "         page_href1.append(a.get('href'))#find all hrefs in those <a> tags\n",
        "for res1 in page_href1:\n",
        "    if res1[39]!='w':\n",
        "     if res1[40]!='a':\n",
        "      if res1[41]!='t':\n",
        "       if res1[42]!='c':\n",
        "        if res1[43]!='h':\n",
        "         page_href.append(res1)#append after removing all problematic urls\n",
        "    \n",
        "[all_hrefs.append(x) for x in page_href if x not in all_hrefs]\n",
        "print(len(all_hrefs))#total number of urls=2547,( may vary with time due to updates )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_t3XJeRyuhY"
      },
      "source": [
        "id=0\n",
        "for url11 in all_hrefs:#url takes the URL for a particular news\n",
        "  data11=[]#stores data for the particular URL\n",
        "  page11= requests.get(url11)\n",
        "  soup11 = BeautifulSoup(page11.content,'html.parser')\n",
        "  table = soup11.findAll('div',attrs={\"class\":\"paywall\"})\n",
        "  if(len(table)==1):\n",
        "   id=id+1\n",
        "   k11=table[0].find_all(\"p\")\n",
        "   k22= table[0].find_all('h5')\n",
        "   s=''\n",
        "   s1=''\n",
        "   for k2 in k11:\n",
        "      s=s+k2.get_text()\n",
        "   for k3 in k22:\n",
        "      s1=s1+k2.get_text()\n",
        "   data11.append(id)\n",
        "   data11.append(s.replace(s1,''))\n",
        "   all_data.append(data11)  \n",
        "   \n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU9-pmosaHYa"
      },
      "source": [
        "fields = ['Id','Content']\n",
        "with open('News_data.csv','w') as news:\n",
        "  write = csv.writer(news)\n",
        "  write.writerow(fields) \n",
        "  write.writerows(all_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}